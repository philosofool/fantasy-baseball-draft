{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sandbox\n",
    "\n",
    "This is a top level notebook for developing and playing with code in the local directory.\n",
    "\n",
    "The expected use is not generally to share code, but to have a space for notebook activity of exploratory nature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.append(r'/home/lenhart/Repos/phi-utils')\n",
    "\n",
    "from philosofool.data_sources.utils import read_yml\n",
    "from fantasy_baseball_draft.utils import StatSynonyms, load_cbs_data, DataLoader\n",
    "\n",
    "data_path = read_yml('local/config.yml')['paths']['local_data']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AssociatePlayers:\n",
    "    \"\"\"Associate entities from different datasets.\n",
    "\n",
    "    Given two dataframes that share entities but not a common single\n",
    "    join key, this can construct a merge of the data based on columns\n",
    "    that are assumed to find unique one.\n",
    "\n",
    "    Example: Given dataframes of pitchers in 2019 that use different\n",
    "    naming conventions ('Mike', 'Michael', etc.) we can align these by \n",
    "    assuming that two pitchers are the same if they started the same\n",
    "    number of games, had the same number of walks, hits and strikeouts.\n",
    "    \"\"\"\n",
    "    syns = StatSynonyms()\n",
    "\n",
    "    def associate(self, df1, df2, index_cols):\n",
    "        if df1.duplicated().sum():\n",
    "            warnings.warn(\"Found duplicated in df1.\")\n",
    "        if df2.duplicated().sum():\n",
    "            warnings.warn(\"Found duplicated in df2.\")\n",
    "        df1 = self.syns.normalize_df(df1)\n",
    "        df2 = self.syns.normalize_df(df2)\n",
    "        df = df1.set_index(index_cols).merge(df2.set_index(index_cols), left_index=True, right_index=True)\n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "associate = AssociatePlayers()\n",
    "#\n",
    "\n",
    "index_cols = ['H', 'BB', 'RBI', 'K']\n",
    "index_cols = ['IP', 'W', 'G', 'K']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# syns = AssociatePlayers.syns\n",
    "\n",
    "loader = DataLoader(os.path.join(data_path, 'historical'))\n",
    "fg22 = loader.load_csv('fg_hitters_2022.csv')\n",
    "cbs22 = loader.load_cbs_csv('cbs_hitters_2022.csv')\n",
    "fg22[fg22.Name.str.contains('France')]\n",
    "#cbs22[cbs22.Player.str.contains('France')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_id_map(cbs_path, fg_path, index_cols, duplicated=False):\n",
    "    data_path = read_yml('local/config.yml')['paths']['local_data']\n",
    "    hist_path = os.path.join(data_path, 'historical')\n",
    "    loader = DataLoader(hist_path)\n",
    "    cbs = loader.load_cbs_csv(cbs_path)\n",
    "    fg = loader.load_csv(fg_path)\n",
    "    \n",
    "    cbs = cbs[cbs[index_cols].sum(axis=1) > 0]\n",
    "    org = len(cbs)\n",
    "    if duplicated:\n",
    "        return cbs[cbs.duplicated(subset=index_cols, keep=False)]\n",
    "    cbs = cbs.drop_duplicates(subset=index_cols, keep=False)\n",
    "    print(f\"Dropped {org - len(cbs)} records.\")\n",
    "\n",
    "    cbs_to_fg = associate.associate(cbs, fg, index_cols)\n",
    "    cbs_to_fg = dict(zip(cbs_to_fg.Player.str.strip(), cbs_to_fg.playerid))\n",
    "    return cbs_to_fg\n",
    "\n",
    "# import json\n",
    "# with open(data_path + '/hitter_ids.json', 'w') as j:\n",
    "#     j.write(json.dumps(cbs_to_fg_h))\n",
    "\n",
    "pitcher_ids = build_id_map('cbs_pitchers_2022.csv', 'fg_pitchers_2022.csv', ['IP', 'W', 'G', 'K'])\n",
    "hitter_ids = build_id_map('cbs_hitters_2022.csv', 'fg_hitters_2022.csv', ['H', 'BB', 'RBI', 'K'])\n",
    "#build_id_map('cbs_hitters_2022.csv', 'fg_hitters_2022.csv', ['H', 'BB', 'RBI', 'K'], True)\n",
    "{k: v for k, v in hitter_ids.items() if 'Franc' in k}# [hitter_ids..str.contains('Franc')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projections = DataLoader(os.path.join(data_path, 'projections'))\n",
    "cbs_hitters = projections.load_cbs_csv('cbs_hitters.csv')\n",
    "cbs_pitchers = projections.load_cbs_csv('cbs_pitchers.csv')\n",
    "print(data_path)\n",
    "fg_hitters = projections.load_csv('fg-depth-chart_hitters.csv')\n",
    "fg_pitcher = projections.load_csv('fg-depth-chart_pitchers.csv')\n",
    "\n",
    "cbs_hitters['playerid'] = cbs_hitters.Player.map(hitter_ids).fillna(-1).astype(int)\n",
    "cbs_pitchers['playerid'] = cbs_pitchers.Player.map(pitcher_ids).fillna(-1).astype(int)\n",
    "cbs_hitters['PA'] = cbs_hitters.AB + cbs_hitters.BB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fg_hitters[fg_hitters.Name.str.contains(\"Tatis\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hitter_ids['Ty France 1B | SEA'] = 17982\n",
    "hitter_ids[\"Fernando Tatis SS | SD\"] = 19709"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbs_pitchers['ER'] = cbs_pitchers.ERA * cbs_pitchers.IP / 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_fg(cbs, base_path, cols, name_filter=lambda x: x):\n",
    "    cbs = cbs.copy()\n",
    "    ids = cbs.playerid.to_numpy()\n",
    "    loader = DataLoader(base_path)\n",
    "    for path in os.listdir(base_path):\n",
    "        if name_filter(path) and 'cbs' not in path:\n",
    "            #print(path)\n",
    "            suffixes = ('', '_' + path[:5].strip('_'))\n",
    "            df = loader.load_csv(path)\n",
    "            if len(df) == 0:\n",
    "                #print(f'{path} data is empty...')\n",
    "                continue\n",
    "            #print(df.columns, df.playerid[:10])\n",
    "            df = df[~df.playerid.astype(str).str.startswith('sa')]\n",
    "\n",
    "            df.playerid = df.playerid.astype(int)\n",
    "            df = df[df.playerid.isin(list(ids))]\n",
    "            assert len(df) > 0, 'df empty, which is sort of like wdf?'\n",
    "            cbs = cbs.merge(df[cols], on='playerid', suffixes=suffixes, how='left')\n",
    "    return cbs\n",
    "\n",
    "def merge_fg_hitters(cbs, base_path):\n",
    "    cols = ['playerid', 'PA',  'AB', 'H', 'HR', 'R', 'RBI', 'SB']\n",
    "    return merge_fg(cbs, base_path, cols, lambda x: 'hitter' in x)\n",
    "\n",
    "def merge_fg_pitchers(cbs, base_path):\n",
    "    cols = ['playerid', 'IP', 'ER', 'WHIP', 'K', 'S', 'W']\n",
    "    return merge_fg(cbs, base_path, cols, lambda x: 'pitch' in x)\n",
    "\n",
    "merge_fg_hitters(cbs_hitters, data_path + '/projections')\n",
    "merge_fg_pitchers(cbs_pitchers, data_path + '/projections')\n",
    "#cbs_pitchers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_weighted_stats(df, stats, playtime_stat):\n",
    "    df = df.copy()\n",
    "    pt = playtime_stat\n",
    "    suffixes = set(['_'.join(col.split('_')[1:]) for col in df.columns if len(col.split('_')) > 1])\n",
    "    df[f'{pt}_adj'] = (df[f'{pt}_fg-de'] + df[pt]) / 2\n",
    "\n",
    "    for suf in suffixes:\n",
    "        for col in stats:\n",
    "            df[f'{col}_{suf}'] = df[f'{col}_{suf}'] / df[f'{pt}_{suf}'] * df[f'{pt}_adj']\n",
    "    for col in stats:\n",
    "        adj_cols = [col] + [c for c in df.columns if re.match(f'{col}_', c)]\n",
    "        df[f'{col}_adj'] = (df[adj_cols].sum(axis=1) / df[adj_cols].notna().sum(axis=1))\n",
    "    return df\n",
    "\n",
    "#add_weighted_stats(cbs_hitters, cols[2:], 'PA')\n",
    "hitter_stats = ['playerid', 'PA',  'AB', 'H', 'HR', 'R', 'RBI', 'SB']\n",
    "pitcher_stats = ['playerid', 'IP', 'ER', 'WHIP', 'K', 'S', 'W']\n",
    "full_hitters = (cbs_hitters\n",
    "    .pipe(merge_fg_hitters, data_path + '/projections')\n",
    "    .pipe(add_weighted_stats, hitter_stats[2:], 'PA'))\n",
    "full_pitchers = (cbs_pitchers\n",
    "    .pipe(merge_fg_pitchers, data_path + '/projections')\n",
    "    .pipe(add_weighted_stats, pitcher_stats[2:], 'IP')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fantasy_baseball_draft.spg import FantasyValuator, spgs_from_standings_html\n",
    "\n",
    "valuator = FantasyValuator(spgs_from_standings_html(os.path.join(data_path, 'standings/cbs_2021_standings.html')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elig = DataLoader(os.path.join(data_path, 'eligibility')).load_cbs_csv('eligibility.csv')\n",
    "\n",
    "def extract_projection(df):\n",
    "    extract = ['Avail', 'Player', 'playerid'] + [col for col in df.columns if '_adj' in col]\n",
    "    df = df[extract].rename(columns={k: re.sub('_adj', '', k) for k in extract})\n",
    "    return df\n",
    "\n",
    "hitter_proj = extract_projection(full_hitters)\n",
    "hitter_proj = hitter_proj.merge(elig[['Player', 'Eligible']],  how='left', on='Player')\n",
    "hitter_proj['fwar'] = valuator.valuate_hitters(hitter_proj, 16*12)\n",
    "\n",
    "pitcher_proj = extract_projection(full_pitchers)\n",
    "pitcher_proj['ERA'] = pitcher_proj.ER / pitcher_proj.IP * 9\n",
    "pitcher_proj['fwar'] = valuator.valuate_pitchers(pitcher_proj, 16*12)\n",
    "\n",
    "hitter_proj.sort_values('fwar', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hitter_proj.to_csv('local/hitter_proj.csv', index_label='index')\n",
    "pitcher_proj.to_csv('local/pitcher_proj.csv', index_label='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "02ee4401c5460a4ee3bc94108cf95546c8e39c298a8555b54bdd3e60e7d4869e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
