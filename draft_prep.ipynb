{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Draft Prep\n",
    "\n",
    "Lots prepare csv files for draft activity. Also prepare weighted csv files for other uses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "from functools import partial\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import re\n",
    "from collections.abc import Callable, Iterable, Mapping\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy.typing import ArrayLike\n",
    "from scipy.stats import rankdata\n",
    "\n",
    "from philosofool.data_sources.utils import read_yml  # type: ignore\n",
    "from philosofool.data_science.graph import MetricGraph\n",
    "\n",
    "from fantasy_baseball_draft.spg import position_adjusted_fwar, largest, spg_model\n",
    "from fantasy_baseball_draft.utils import StatSynonyms, load_cbs_data, DataLoader\n",
    "from fantasy_baseball_draft.utils import cbs_player_col_to_df\n",
    "from fantasy_baseball_draft.stats import StatCalculator\n",
    "from fantasy_baseball_draft.draft_prep.align import build_id_map_from_stat_associations, build_id_map\n",
    "\n",
    "data_path = read_yml('local/config.yml')['paths']['local_data']\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ID Functions\n",
    "\n",
    "Why is this so complicated? We need to join players expected to play in the current year with Fangraphs ID data to create a unique ID column for merging.\n",
    "TL;DR: we're working with disjoint lists to make it all work.\n",
    "Historical data solves this problem simply for players who have played.\n",
    "But there are players expected who did not appear in the previous year.\n",
    "For those, the name of the player is helpful for the merging.\n",
    "But the result is that we need (1) a dataset of historical MLB play and (2) some preseason projections.\n",
    "The players "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pitching_metrics = ['ERA', 'WHIP', 'W', 'S', 'K']\n",
    "hitting_metrics = ['R', 'HR', 'RBI', 'SB', 'BA']\n",
    "scoring_metrics = pitching_metrics + hitting_metrics\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Fangraphs Player Id to CBS data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hitter_match = ['AB', 'H', 'BB', 'RBI', 'K']\n",
    "pitcher_match = ['IP', 'W', 'G', 'K', 'H', 'W']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = read_yml('local/config.yml')['paths']['local_data']\n",
    "hist_path = os.path.join(data_path, 'historical')\n",
    "loader = DataLoader(hist_path)\n",
    "\n",
    "pitcher_ids = build_id_map_from_stat_associations(\n",
    "    loader.load_cbs_csv('cbs_pitchers_2023.csv'),\n",
    "    loader.load_csv('fg_pitchers_2023.csv'),\n",
    "    pitcher_match\n",
    ")\n",
    "\n",
    "hitter_ids = build_id_map_from_stat_associations(\n",
    "    loader.load_cbs_csv('cbs_hitters_2023.csv'),\n",
    "    loader.load_csv('fg_hitters_2023.csv'),\n",
    "    hitter_match\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_stats(df1, df2, stats: list[str]) -> pd.DataFrame:\n",
    "    \"\"\"Combine stats from two dataframes.\"\"\"\n",
    "\n",
    "    if not len(df1.columns.intersection(stats)) == len(stats):\n",
    "        raise ValueError(f\"Some requested stats are not in df1 {df1}\")\n",
    "    if not len(df2.columns.intersection(stats)) == len(stats):\n",
    "        raise ValueError(f\"Some requested stats are not in df2 {df2}\")\n",
    "\n",
    "    shared_idx = df2.index.intersection(df1.index)\n",
    "    out = df1.copy()[stats].astype({col: float for col in stats})\n",
    "    out.loc[shared_idx, stats] = (df1[stats] + df2[stats]) / 2\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_weight_stats():\n",
    "    df1 = pd.DataFrame({'a': [1, 3, 0], 'b': [0, 1, 2], 'c': [0, 0, 0]}, index=[1, 4, 2]).astype(float)\n",
    "    df2 = pd.DataFrame({'a': [1, 1, 3], 'b': [0, 1, 2]}, index=[1, 2, 3]).astype(float)\n",
    "    result = weight_stats(df1, df2, ['a', 'b'])\n",
    "\n",
    "    assert isinstance(result, pd.DataFrame)\n",
    "    assert result['a'].loc[2] == 0.5\n",
    "    assert 'c' not in result\n",
    "\n",
    "\n",
    "test_weight_stats()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def as_int_safe(idx):\n",
    "    if isinstance(idx, str):\n",
    "        try:\n",
    "            return int(idx)\n",
    "        except Exception:\n",
    "            return idx\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_missing_pitchers(df: pd.DataFrame):\n",
    "    df = df.copy()\n",
    "    # Fix Edwin Diaz\n",
    "    # TODO: figure out why he wasn't found. Probably the diacritic in his last name.\n",
    "    assert 'Edwin Diaz' in df.loc[15].Player\n",
    "    df.loc[15, 'playerid'] = 14710\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eligibility = DataLoader(os.path.join(data_path, 'eligibility')).load_cbs_csv('eligibility.csv')\n",
    "projections = DataLoader(os.path.join(data_path, 'projections/2024'))\n",
    "fg_hitter_proj = projections.load_csv('fg_depth_hitters.csv').map(as_int_safe)\n",
    "fg_pitcher_proj = projections.load_csv('fg_depth_pitchers.csv').map(as_int_safe)\n",
    "\n",
    "cbs_hitters_proj = (\n",
    "    projections\n",
    "    .load_cbs_csv('cbs_hitters.csv')\n",
    "    .drop(columns=['BA'])  # recalculate later.\n",
    "    .assign(\n",
    "        playerid=lambda df: build_id_map(df, fg_hitter_proj, hitter_ids),\n",
    "        PA=lambda df: df.AB + df.BB\n",
    "    )\n",
    "    .merge(eligibility[[\"Player\", \"Eligible\"]], how='left', on=\"Player\")\n",
    "    .merge(\n",
    "        projections.load_csv('age_data.csv').drop_duplicates(subset='playerid').filter(['playerid', 'Age']),\n",
    "        on='playerid',\n",
    "        how='left'\n",
    "    )\n",
    "    .fillna({'Age': 25})\n",
    ")\n",
    "\n",
    "cbs_pitchers_proj = (\n",
    "    projections\n",
    "    .load_cbs_csv('cbs_pitchers.csv')\n",
    "    .assign(\n",
    "        playerid=lambda df: build_id_map(df, fg_pitcher_proj, pitcher_ids),\n",
    "        ER=lambda df: df.ERA * df.IP / 9\n",
    "    )\n",
    "    .drop(columns=['ERA', 'WHIP'])  # recalculate later.\n",
    ")\n",
    "\n",
    "def _drop_dups_set_index(df, col: str|list[str]) -> pd.DataFrame:\n",
    "    return df.drop_duplicates(subset=col).set_index(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_hitter_draft_data(cbs_data: pd.DataFrame, fangraphs_data: pd.DataFrame, eligibility: pd.DataFrame, age_data: pd.DataFrame, hitter_ids: pd.Series):\n",
    "    cbs_hitters_proj = (\n",
    "        cbs_data\n",
    "        .drop(columns=['BA'])  # recalculate later.\n",
    "        .assign(\n",
    "            playerid=lambda df: build_id_map(df, fangraphs_data, hitter_ids),\n",
    "            PA=lambda df: df.AB + df.BB\n",
    "        )\n",
    "        .merge(eligibility[[\"Player\", \"Eligible\"]], how='left', on=\"Player\")\n",
    "        .merge(\n",
    "            age_data.drop_duplicates(subset='playerid').filter(['playerid', 'Age']),\n",
    "            on='playerid',\n",
    "            how='left'\n",
    "        )\n",
    "        .fillna({'Age': 25})\n",
    "    )\n",
    "\n",
    "    hitters = (\n",
    "        cbs_hitters_proj\n",
    "        .set_index('playerid')\n",
    "        .assign(**weight_stats(\n",
    "            cbs_hitters_proj.drop_duplicates(subset='playerid').set_index('playerid'),\n",
    "            fg_hitter_proj.set_index('playerid'),\n",
    "            ['PA', 'AB', 'BB', 'H', 'HR', 'K', 'RBI', 'R', 'SB'])\n",
    "        )\n",
    "    )\n",
    "    return hitters\n",
    "\n",
    "def make_pitcher_draft_data(cbs_data: pd.DataFrame, fangraphs_data: pd.DataFrame, pitcher_ids: pd.Series):\n",
    "    cbs_pitchers_proj = (\n",
    "        cbs_data\n",
    "        .assign(\n",
    "            playerid=lambda df: build_id_map(df, fangraphs_data, pitcher_ids),\n",
    "            ER=lambda df: df.ERA * df.IP / 9\n",
    "        )\n",
    "        .drop(columns=['ERA', 'WHIP'])  # recalculate later.\n",
    "        .pipe(add_missing_pitchers)\n",
    "    )\n",
    "\n",
    "    pitchers = (\n",
    "        cbs_pitchers_proj\n",
    "        .set_index('playerid')\n",
    "        .assign(**weight_stats(cbs_pitchers_proj.drop_duplicates('playerid').set_index('playerid'), fg_pitcher_proj.set_index('playerid'), ['IP', 'G', 'GS', 'QS', 'W', 'L', 'S', 'K', 'BB', 'H', 'ER']))\n",
    "    )\n",
    "    return pitchers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbs_pitchers_proj.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hitters\n",
    "pitchers.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Fantasy League Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positional eligibilty and positional value\n",
    "\n",
    "def largest(arr: np.ndarray, k: int) -> np.ndarray:\n",
    "    \"\"\"Return the k largest elements in an array.\n",
    "\n",
    "    The output is unsorted, but element 0 is its minimum value.\n",
    "    \"\"\"\n",
    "    k_ = arr.size - k\n",
    "    return np.partition(arr, kth=k_)[-k:]\n",
    "\n",
    "def position_value(hitter_fwar: ArrayLike, eligible: ArrayLike, position: str, n_rostered: int) -> float:\n",
    "    if hasattr(hitter_fwar, 'values') :\n",
    "        hitter_fwar = hitter_fwar.values\n",
    "    eligible_idx = np.nonzero(matches_eligible(eligible, position))\n",
    "    best = np.partition(hitter_fwar[eligible_idx], kth=n_rostered)[-n_rostered:]\n",
    "    return best[0]\n",
    "\n",
    "def matches_eligible(eligbible: ArrayLike, position: str) -> pd.Series:\n",
    "    \"\"\"Return boolean series where eligible matches position.\"\"\"\n",
    "    elig_series = pd.Series(eligbible)\n",
    "    pattern = f'^{position}$|,{position},|^{position},|,{position}$'\n",
    "    clean_elig = elig_series.str.replace(r'\\s', '', regex=True)\n",
    "    return clean_elig.str.contains(pattern, regex=True)\n",
    "\n",
    "\n",
    "def position_adjusted_fwar(raw_fwar: ArrayLike, eligible: ArrayLike, position: str, pos_rostered: int, roster_depth: int) -> np.ndarray:\n",
    "    \"\"\"Model position adjusted fantasy value.\"\"\"\n",
    "    # TODO: Figure out how to link graphs so that positonal valuation can pass through more efficiently:\n",
    "    # this process won't cache positional value during the calculations, so multiple calls repeat the\n",
    "    # baseline call.\n",
    "    raw_fwar = raw_fwar.fillna(-1.)\n",
    "    baseline_replacement_level = largest(raw_fwar, roster_depth)[0]\n",
    "    position_replacement_level = position_value(raw_fwar, eligible, position, pos_rostered)\n",
    "    pos_value = baseline_replacement_level - position_replacement_level\n",
    "    return np.where(matches_eligible(eligible, position), raw_fwar + pos_value - baseline_replacement_level, raw_fwar - baseline_replacement_level)\n",
    "\n",
    "# some tests\n",
    "\n",
    "def test_matches_eligible():\n",
    "    \"\"\"Test of matches_eligible.\"\"\"\n",
    "    eligible = ['C', 'C, CF', '1B, C', '1B, C, SS', 'CF', 'SS']\n",
    "    result = matches_eligible(eligible, 'C')\n",
    "    assert np.array_equal(result, [True, True, True, True, False, False])\n",
    "\n",
    "def test_postion_value():\n",
    "    \"\"\"Test of postiion_values.\"\"\"\n",
    "    fwar = np.arange(11, 0, -1) + 2\n",
    "    eligible = np.array(('c' + ' 1b'*6 + ' c'*4).split())\n",
    "    result = position_value(fwar, eligible, 'c', 4)\n",
    "    assert result == 3\n",
    "    result = position_value(fwar, eligible, 'c', 1)\n",
    "    assert result == 13\n",
    "    np.testing.assert_raises(ValueError, position_value, fwar, eligible, 'c', 10)\n",
    "\n",
    "    fwar = pd.Series(fwar)  # series may not play with internals of position_value.\n",
    "    result = position_value(fwar, eligible, 'c', 4)\n",
    "    assert result == 3\n",
    "\n",
    "\n",
    "test_matches_eligible()\n",
    "test_postion_value()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def standings_html_to_df(standings: list[pd.DataFrame]) -> pd.DataFrame:\n",
    "    \"\"\"Combine several tables extracted from a webpage into a single DataFrame.\"\"\"\n",
    "    stat_standings = []\n",
    "    for df in standings:\n",
    "        metrics = StatCalculator().metrics\n",
    "        for i, row in df.iterrows():\n",
    "            if row.loc[0] == 'Team':\n",
    "                idx = i\n",
    "                break\n",
    "        stat_standings.append(df.loc[i + 1:, 0:1].rename(columns=df.loc[i]))\n",
    "    standings = functools.reduce(lambda a, b: a.merge(b, on='Team'), stat_standings)\n",
    "    return standings.astype({col: float for col in standings.columns if col != 'Team'})\n",
    "\n",
    "\n",
    "def weighted_ratio(x: ArrayLike, weight: ArrayLike, median: ArrayLike):\n",
    "    return weight * (x - median)\n",
    "\n",
    "def model_spg(arr: np.ndarray|pd.Series, low_better=False) -> Callable[[ArrayLike], np.ndarray]:\n",
    "    \"\"\"Calculate a linear regression for spg weights and return a function that applies it.\"\"\"\n",
    "    if isinstance(arr, pd.Series):\n",
    "        arr = arr.values\n",
    "    arr = arr.reshape(-1, 1)\n",
    "    points = rankdata(arr * -1) if low_better else rankdata(arr)\n",
    "    slope = LinearRegression().fit(arr, points).coef_[0]\n",
    "\n",
    "    def spg_value(x: ArrayLike) -> np.ndarray:\n",
    "        return x * slope\n",
    "\n",
    "    return spg_value\n",
    "\n",
    "\n",
    "spg_model = MetricGraph.from_model({\n",
    "    # 'IP': (lambda _: 1200., ('ERA',)),\n",
    "    # 'AB': (lambda _: 5600., ('BA',)),\n",
    "    'median_ERA': (np.median, ('ERA',)),\n",
    "    'median_WHIP': (np.median, ('WHIP',)),\n",
    "    'median_BA': (np.median, ('BA',)),\n",
    "    'xER': (weighted_ratio, ('ERA', 'IP', 'median_ERA')),\n",
    "    'xWHIP': (weighted_ratio, ('WHIP', 'IP', 'median_WHIP')),\n",
    "    'xH': (weighted_ratio, ('BA', 'AB', 'median_BA')),\n",
    "    'W_spg': (model_spg, ('W',)),\n",
    "    'S_spg': (model_spg, ('S',)),\n",
    "    'K_spg': (model_spg, ('K',)),\n",
    "    'ERA_spg': (partial(model_spg, low_better=True), ('xER',)),\n",
    "    'WHIP_spg': (partial(model_spg, low_better=True), ('xWHIP',)),\n",
    "    'R_spg': (model_spg, ('R',)),\n",
    "    'HR_spg': (model_spg, ('HR',)),\n",
    "    'RBI_spg': (model_spg, ('RBI',)),\n",
    "    'SB_spg': (model_spg, ('SB',)),\n",
    "    'BA_spg': (model_spg, ('xH',)),\n",
    "})\n",
    "\n",
    "def extract_model(df: pd.DataFrame, metric_graph: MetricGraph, metrics: Iterable) -> MetricGraph:\n",
    "    \"\"\"Convert standings to a metric graph for subsequent calculations.\"\"\"\n",
    "    calculated_model = metric_graph.calculate_metrics(df, metrics)\n",
    "    model_graph = {metric: metric_graph.dependency_graph[metric] for metric in metrics}\n",
    "    model_fns = {metric: calculated_model[metric] for metric in metrics}\n",
    "    return MetricGraph(model_graph, model_fns)\n",
    "\n",
    "\n",
    "fwar_model = {\n",
    "    'pitcher_raw_fWAR': (StatCalculator.reduce_sum, tuple(f'{metric}_spg' for metric in pitching_metrics)),\n",
    "    'pitcher_fWAR': (lambda fwar: fwar - largest(fwar.fillna(-1), 160)[0], ('pitcher_raw_fWAR',)),\n",
    "    'pitcher_fWAR150': (lambda raw, ip: np.divide(raw * 150., ip, where=(ip!= 0.), out=np.zeros_like(ip, dtype=np.float32)), ('pitcher_raw_fWAR', 'IP')),\n",
    "    'hitter_raw_fWAR': (StatCalculator.reduce_sum, ('R_spg', 'RBI_spg', 'HR_spg', 'BA_spg', 'SB_spg')),\n",
    "    'hitter_fWAR': (partial(position_adjusted_fwar, position='C', pos_rostered=16, roster_depth=176), ('hitter_raw_fWAR', 'Eligible')),\n",
    "    'hitter_fWAR600':(lambda raw, pa: np.divide(raw * 600, pa, where=(pa!= 0.), out=np.zeros_like(pa, dtype=np.float32)), ('hitter_raw_fWAR', 'PA')),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# a bit of boilerplate with known values.\n",
    "standings = standings_html_to_df(pd.read_html(os.path.join(data_path, 'standings/klf_2023.html'))[5:])\n",
    "spg_names = [f'{metric}_spg' for metric in pitching_metrics + hitting_metrics]\n",
    "\n",
    "fantasy_stat_model = StatCalculator.from_model(\n",
    "    extract_model(standings.assign(AB=lambda _: 5600, IP=lambda _: 1200), spg_model, spg_names).model()\n",
    "    | spg_model.model(['xER', 'xH', 'xWHIP'])\n",
    "    | fwar_model\n",
    "    | StatCalculator().model\n",
    ")\n",
    "median_stats = spg_model.calculate_metrics(standings, ['median_BA', 'median_ERA', 'median_WHIP'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hitters = fantasy_stat_model.add_metrics(hitters.assign(median_BA=lambda _: median_stats['median_BA']), metrics=['BA', 'hitter_fWAR', 'hitter_fWAR600'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pitchers = fantasy_stat_model.add_metrics(\n",
    "    pitchers.assign(\n",
    "        median_ERA=lambda _: median_stats['median_ERA'],\n",
    "        median_WHIP=lambda _: median_stats['median_WHIP']\n",
    "    ),\n",
    "    metrics=[f'{metric}_spg' for metric in pitching_metrics] + ['ERA', 'WHIP', 'pitcher_fWAR', 'pitcher_fWAR150']\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Closers\n",
    "\n",
    "Which live in their own separate world."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_ids(original_id) -> int:\n",
    "    \"\"\"Fix the datatype where some there are alpha numeric and integers in the id column.\"\"\"\n",
    "    if type(original_id) == str:\n",
    "        if re.search(r'[A-Za-z].', original_id):\n",
    "            id = int(re.sub(r'[A-Za-z].', '', original_id))\n",
    "            return id\n",
    "    elif pd.isna(original_id):\n",
    "        return -1\n",
    "    return int(original_id)\n",
    "\n",
    "\n",
    "def cummulative_rank(df):\n",
    "    # TODO: less lazy version where we at least pass the rank column names.\n",
    "    stacked = np.stack([df[col] for col in ('Eno', 'Greg', 'lenhart_rank')], axis=1)\n",
    "    stacked = np.where(stacked <= 40, 40 - stacked, 0.)\n",
    "    stacked = np.sqrt(np.sum(stacked ** 2, axis=1))\n",
    "    return rankdata(-1*stacked)\n",
    "\n",
    "closer_data = (\n",
    "    (athlethic_projections := projections.load_csv('the_athletic_closers.csv'))\n",
    "\n",
    "    .merge(\n",
    "        fg_pitcher_proj,\n",
    "        how='left',\n",
    "        on='Name',\n",
    "    )\n",
    "    .assign(playerid=lambda df: df.playerid.apply(lambda s: fix_ids(s)))\n",
    "    .filter(regex=r'Name|Eno|Greg|ADP|playerid')\n",
    "    .astype({'playerid': int}).set_index('playerid')\n",
    "    .merge(pitchers.filter(pitching_metrics + ['pitcher_fWAR']), right_index=True, left_index=True, how='left')\n",
    "    .dropna()\n",
    "    .assign(\n",
    "        lenhart_rank=lambda df: rankdata(df.pitcher_fWAR.values * -1),\n",
    "        closer_rank=cummulative_rank\n",
    "    )\n",
    "    .filter(['playerid', 'closer_rank', 'Eno', 'Greg'])\n",
    ")\n",
    "\n",
    "pitchers = pitchers.merge(closer_data, left_index=True, right_index=True, how='left').fillna(-1)#.query('closer_rank > 0')\n",
    "#closer_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hitter_essentials = ['Avail', 'Player', 'Age', 'Eligible', 'PA', 'R', 'HR', 'RBI', 'K', 'SB', 'BA', 'hitter_fWAR', 'hitter_fWAR600']\n",
    "pitcher_essentials = ['Avail', 'Player', 'IP', 'GS', 'W', 'S', 'K', 'ERA', 'WHIP', 'pitcher_fWAR', 'pitcher_fWAR150', 'closer_rank']\n",
    "\n",
    "hitter_df = hitters[hitter_essentials].sort_values('hitter_fWAR', ascending=False)\n",
    "pitcher_df = pitchers[pitcher_essentials].sort_values('pitcher_fWAR', ascending=False)\n",
    "\n",
    "def filter_avail(df, regex):\n",
    "    return df[df.Avail.str.contains(regex)]\n",
    "\n",
    "def filter_elig(df, position):\n",
    "    return df[matches_eligible(df.Eligible, position)]\n",
    "\n",
    "def filter_name(df, regex):\n",
    "    return df[df.Player.str.contains(regex, re.IGNORECASE)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fresh_pitch = DataLoader(os.path.join(data_path, 'draft')).load_cbs_csv('pitchers.csv').Player.tolist()\n",
    "# fresh_hit = DataLoader(os.path.join(data_path, 'draft')).load_cbs_csv(...).Player\n",
    "pitcher_df[pitcher_df.Player.isin(fresh_pitch)]\n",
    "# hitter_df[hitter_df.Player.isin(fresh_hit)]\n",
    "# fresh_pitch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hitter_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "position = 'CI|MI'\n",
    "avail = 'W\\W'\n",
    "name = ''\n",
    "start = 5\n",
    "hitter_df.pipe(filter_avail, avail).pipe(filter_elig, position).pipe(filter_name, name)[start:start+15].sort_values('hitter_fWAR', ascending=False)\n",
    "# hitter_df.query('Age < 25').pipe(filter_avail, avail)[start:start+15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = ''\n",
    "avail = 'W\\W'\n",
    "name = ''\n",
    "start = 28\n",
    "closers = False\n",
    "\n",
    "\n",
    "def pitcher_filter(df, avail: str, query: str, closers: bool, name: str, start: int):\n",
    "    stop = start + 15\n",
    "    _filter_closers = 'closer_rank != -1 or S > 0'\n",
    "    _q_string = _filter_closers if closers else query\n",
    "    _sort_by = 'pitcher_fWAR' if not closers else 'closer_rank'\n",
    "    ascend = True if closers else False\n",
    "    pitchers = (\n",
    "        pitcher_df\n",
    "        .filter(pitcher_essentials)\n",
    "        .pipe(filter_avail, avail)\n",
    "    )\n",
    "    if query or closers:\n",
    "        pitchers = pitchers.query(_q_string)\n",
    "    return pitchers.pipe(filter_name, name)[start:stop].sort_values(_sort_by, ascending=ascend)\n",
    "\n",
    "pitcher_filter(pitcher_df, avail, query, False, name, start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pitcher_filter(pitcher_df, avail, query, True, name, start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pitchers.columns\n",
    "hitters[hitter_essentials].pipe(filter_avail, '^W[^a-z]')\n",
    "hitters[hitter_essentials].pipe(filter_elig, 'U').pipe(filter_avail, '^W[^a-z]').head(25)[['Avail', 'Age', 'Player', 'Eligible', 'hitter_fWAR']].sort_values('hitter_fWAR', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positions = ['C', '1B', '2B', '3B', 'SS', 'CF', 'LF', 'RF', 'U']\n",
    "for position in positions:\n",
    "    position_df = (\n",
    "        hitters\n",
    "        .pipe(filter_elig, position)\n",
    "        .sort_values('hitter_fWAR', ascending=False)\n",
    "        .filter(hitter_essentials)\n",
    "        .pipe(filter_avail, '^W[^a-z]')\n",
    "        .query('hitter_fWAR > .1 or (hitter_fWAR600 > 15 and PA > 400)')\n",
    "        #.drop(columns='Avail')\n",
    "        .round(3)\n",
    "    )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matching Unfound Players\n",
    "\n",
    "Everything above is solid. WIP stuff to find more player ids."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "02ee4401c5460a4ee3bc94108cf95546c8e39c298a8555b54bdd3e60e7d4869e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
