{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Draft Prep\n",
    "\n",
    "Lots prepare csv files for draft activity. Also prepare weighted csv files for other uses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "from functools import partial\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import re\n",
    "from collections.abc import Callable, Iterable, Mapping\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy.typing import ArrayLike\n",
    "from scipy.stats import rankdata\n",
    "\n",
    "from philosofool.data_sources.utils import read_yml  # type: ignore\n",
    "from philosofool.data_science.graph import MetricGraph\n",
    "from fantasy_baseball_draft.utils import StatSynonyms, load_cbs_data, DataLoader\n",
    "from fantasy_baseball_draft.utils import cbs_player_col_to_df\n",
    "from fantasy_baseball_draft.stats import StatCalculator\n",
    "from fantasy_baseball_draft.draft_prep.align import build_id_map_from_stat_associations, build_id_map\n",
    "\n",
    "data_path = read_yml('local/config.yml')['paths']['local_data']\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ID Functions\n",
    "\n",
    "Why is this so complicated? We need to join players expected to play in the current year with Fangraphs ID data to create a unique ID column for merging.\n",
    "TL;DR: we're working with disjoint lists to make it all work.\n",
    "Historical data solves this problem simply for players who have played.\n",
    "But there are players expected who did not appear in the previous year.\n",
    "For those, the name of the player is helpful for the merging.\n",
    "But the result is that we need (1) a dataset of historical MLB play and (2) some preseason projections.\n",
    "The players "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pitching_metrics = ['ERA', 'WHIP', 'W', 'S', 'K']\n",
    "hitting_metrics = ['R', 'HR', 'RBI', 'SB', 'BA']\n",
    "scoring_metrics = pitching_metrics + hitting_metrics\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Fangraphs Player Id to CBS data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hitter_match = ['AB', 'H', 'BB', 'RBI', 'K']\n",
    "pitcher_match = ['IP', 'W', 'G', 'K', 'H', 'W']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = read_yml('local/config.yml')['paths']['local_data']\n",
    "hist_path = os.path.join(data_path, 'historical')\n",
    "loader = DataLoader(hist_path)\n",
    "\n",
    "pitcher_ids = build_id_map_from_stat_associations(\n",
    "    loader.load_cbs_csv('cbs_pitchers_2023.csv'),\n",
    "    loader.load_csv('fg_pitchers_2023.csv'), pitcher_match\n",
    ")\n",
    "\n",
    "hitter_ids = build_id_map_from_stat_associations(\n",
    "    loader.load_cbs_csv('cbs_hitters_2023.csv'),\n",
    "    loader.load_csv('fg_hitters_2023.csv'), hitter_match\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_stats(df1, df2, stats: list[str]) -> dict:\n",
    "    \"\"\"Combine stats from two dataframes.\"\"\"\n",
    "\n",
    "    if not len(df1.columns.intersection(stats)) == len(stats):\n",
    "        raise ValueError(f\"Some requested stats are not in df1 {df1}\")\n",
    "    if not len(df2.columns.intersection(stats)) == len(stats):\n",
    "        raise ValueError(f\"Some requested stats are not in df1 {df2}\")\n",
    "\n",
    "    shared_idx = df2.index.intersection(df1.index)\n",
    "    df1_idx = df1.index.difference(shared_idx)\n",
    "    df2_idx = df2.index.difference(shared_idx)\n",
    "\n",
    "    return {\n",
    "        stat: pd.concat([\n",
    "            (df1.loc[shared_idx, stat] + df2.loc[shared_idx, stat]) / 2,\n",
    "            df1.loc[df1_idx, stat],\n",
    "            df2.loc[df2_idx, stat]\n",
    "        ])\n",
    "        for stat in stats\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eligibility = DataLoader(os.path.join(data_path, 'eligibility')).load_cbs_csv('eligibility.csv')\n",
    "projections = DataLoader(os.path.join(data_path, 'projections/2024'))\n",
    "fg_hitter_proj = projections.load_csv('fg_depth_hitters.csv')\n",
    "fg_pitcher_proj = projections.load_csv('fg_depth_pitchers.csv')\n",
    "\n",
    "cbs_hitters_proj = (\n",
    "    projections\n",
    "    .load_cbs_csv('cbs_hitters.csv')\n",
    "    .drop(columns=['BA'])  # recalculate later.\n",
    "    .assign(\n",
    "        playerid=lambda df: build_id_map(df, fg_hitter_proj, hitter_ids),\n",
    "        PA=lambda df: df.AB + df.BB\n",
    "    )\n",
    "    .merge(eligibility[[\"Player\", \"Eligible\"]], how='left', on=\"Player\")\n",
    "    .merge(\n",
    "        projections.load_csv('age_data.csv').drop_duplicates(subset='playerid').filter(['playerid', 'Age']),\n",
    "        on='playerid',\n",
    "        how='left'\n",
    "    )\n",
    "    .fillna({'Age': 25})\n",
    ")\n",
    "\n",
    "cbs_pitchers_proj = (\n",
    "    projections\n",
    "    .load_cbs_csv('cbs_pitchers.csv')\n",
    "    .assign(\n",
    "        playerid=lambda df: build_id_map(df, fg_pitcher_proj, pitcher_ids),\n",
    "        ER=lambda df: df.ERA * df.IP / 9\n",
    "    )\n",
    "    .drop(columns=['ERA', 'WHIP'])  # recalculate later.\n",
    ")\n",
    "\n",
    "# pitcher_forecast_stats = [ 'IP', 'G', 'GS', 'QS', 'W', 'L', 'S', 'K', 'BB', 'H', 'ERA', 'ER', 'WHIP']\n",
    "# hitter_forecast_stats =\n",
    "\n",
    "hitters = cbs_hitters_proj.assign(**weight_stats(cbs_hitters_proj, fg_hitter_proj, ['PA', 'AB', 'BB', 'H', 'HR', 'K', 'RBI', 'R', 'SB']))\n",
    "pitchers = cbs_pitchers_proj.assign(**weight_stats(cbs_pitchers_proj, fg_pitcher_proj, ['IP', 'G', 'GS', 'QS', 'W', 'L', 'S', 'K', 'BB', 'H', 'ER']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbs_pitchers_proj.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbs_hitter_proj.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positional eligibilty and positional value\n",
    "\n",
    "def largest(arr: np.ndarray, k: int) -> np.ndarray:\n",
    "    \"\"\"Return the k largest elements in an array.\n",
    "\n",
    "    The output is unsorted, but element 0 is its minimum value.\n",
    "    \"\"\"\n",
    "    k_ = arr.size - k\n",
    "    return np.partition(arr, kth=k_)[-k:]\n",
    "\n",
    "def position_value(hitter_fwar: ArrayLike, eligible: ArrayLike, pos: str, n_rostered: int) -> float:\n",
    "    if hasattr(hitter_fwar, 'values') :\n",
    "        hitter_fwar = hitter_fwar.values\n",
    "    eligible_idx = np.nonzero(matches_eligible(eligible, pos))\n",
    "    best = np.partition(hitter_fwar[eligible_idx], kth=n_rostered)[-n_rostered:]\n",
    "    return best[0]\n",
    "\n",
    "def matches_eligible(eligbible: ArrayLike, pos: str) -> pd.Series:\n",
    "    elig_series = pd.Series(eligbible)\n",
    "    pattern = f'^{pos}$|,{pos},|^{pos},|,{pos}$'\n",
    "    clean_elig = elig_series.str.replace(r'\\s', '', regex=True)\n",
    "    return clean_elig.str.contains(pattern, regex=True)\n",
    "\n",
    "def test_matches_eligible():\n",
    "    \"\"\"Test of matches_eligible.\"\"\"\n",
    "    eligible = ['C', 'C, CF', '1B, C', '1B, C, SS', 'CF', 'SS']\n",
    "    result = matches_eligible(eligible, 'C')\n",
    "    assert np.array_equal(result, [True, True, True, True, False, False])\n",
    "\n",
    "def test_postion_value():\n",
    "    \"\"\"Test of postiion_values.\"\"\"\n",
    "    fwar = np.arange(11, 0, -1) + 2\n",
    "    eligible = np.array(('c' + ' 1b'*6 + ' c'*4).split())\n",
    "    result = position_value(fwar, eligible, 'c', 4)\n",
    "    assert result == 3\n",
    "    result = position_value(fwar, eligible, 'c', 1)\n",
    "    assert result == 13\n",
    "    np.testing.assert_raises(ValueError, position_value, fwar, eligible, 'c', 10)\n",
    "\n",
    "    fwar = pd.Series(fwar)  # series may not play with internals of position_value.\n",
    "    result = position_value(fwar, eligible, 'c', 4)\n",
    "    assert result == 3\n",
    "\n",
    "\n",
    "test_matches_eligible()\n",
    "test_postion_value()\n",
    "\n",
    "def position_adjusted(fwar: ArrayLike, eligible: ArrayLike, pos: str, pos_roster: int, roster_depth: int) -> np.ndarray:\n",
    "    # TODO: Figure out how to link graphs so that positonal valuation can pass through more efficiently:\n",
    "    # this process won't cache positional value during the calculations, so multiple calls repeat the\n",
    "    # baseline call.\n",
    "    baseline_replacement_level = largest(fwar, roster_depth)[0]\n",
    "    print(baseline_replacement_level)\n",
    "    position_replacement_level = position_value(fwar, eligible, pos, pos_roster)\n",
    "    pos_value = baseline_replacement_level - position_replacement_level\n",
    "    return np.where(matches_eligible(eligible, pos), fwar + pos_value - baseline_replacement_level, fwar - baseline_replacement_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def standings_html_to_df(standings: list[pd.DataFrame]) -> pd.DataFrame:\n",
    "    stat_standings = []\n",
    "    for df in standings:\n",
    "        metrics = StatCalculator().metrics\n",
    "        for i, row in df.iterrows():\n",
    "            if row.loc[0] == 'Team':\n",
    "                idx = i\n",
    "                break\n",
    "        stat_standings.append(df.loc[i + 1:, 0:1].rename(columns=df.loc[i]))\n",
    "    standings = functools.reduce(lambda a, b: a.merge(b, on='Team'), stat_standings)\n",
    "    return standings.astype({col: float for col in standings.columns if col != 'Team'})\n",
    "\n",
    "\n",
    "standings = standings_html_to_df(pd.read_html(os.path.join(data_path, 'standings/klf_2023.html'))[5:])\n",
    "\n",
    "\n",
    "def weighted_ratio(x: ArrayLike, weight: ArrayLike, median: ArrayLike):\n",
    "    return weight * (x - median)\n",
    "\n",
    "def model_spg(arr: np.ndarray|pd.Series, low_better=False) -> Callable[[ArrayLike], np.ndarray]:\n",
    "    \"\"\"Calculate a linear regression for spg weights and return a function that applies it.\"\"\"\n",
    "    if isinstance(arr, pd.Series):\n",
    "        arr = arr.values\n",
    "    arr = arr.reshape(-1, 1)\n",
    "    points = rankdata(arr * -1) if low_better else rankdata(arr)\n",
    "    slope = LinearRegression().fit(arr, points).coef_[0]\n",
    "\n",
    "    def spg_value(x: ArrayLike) -> np.ndarray:\n",
    "        return x * slope\n",
    "\n",
    "    return spg_value\n",
    "\n",
    "\n",
    "spg_model = MetricGraph.from_model({\n",
    "    # 'IP': (lambda _: 1200., ('ERA',)),\n",
    "    # 'AB': (lambda _: 5600., ('BA',)),\n",
    "    'median_ERA': (np.median, ('ERA',)),\n",
    "    'median_WHIP': (np.median, ('WHIP',)),\n",
    "    'median_BA': (np.median, ('BA',)),\n",
    "    'xER': (weighted_ratio, ('ERA', 'IP', 'median_ERA')),\n",
    "    'xWHIP': (weighted_ratio, ('WHIP', 'IP', 'median_WHIP')),\n",
    "    'xH': (weighted_ratio, ('BA', 'AB', 'median_BA')),\n",
    "    'W_spg': (model_spg, ('W',)),\n",
    "    'S_spg': (model_spg, ('S',)),\n",
    "    'K_spg': (model_spg, ('K',)),\n",
    "    'ERA_spg': (partial(model_spg, low_better=True), ('xER',)),\n",
    "    'WHIP_spg': (partial(model_spg, low_better=True), ('xWHIP',)),\n",
    "    'R_spg': (model_spg, ('R',)),\n",
    "    'HR_spg': (model_spg, ('HR',)),\n",
    "    'RBI_spg': (model_spg, ('RBI',)),\n",
    "    'SB_spg': (model_spg, ('SB',)),\n",
    "    'BA_spg': (model_spg, ('xH',)),\n",
    "})\n",
    "\n",
    "def extract_model(df: pd.DataFrame, metric_graph: MetricGraph, metrics: Iterable) -> MetricGraph:\n",
    "    calculated_model = metric_graph.calculate_metrics(df, metrics)\n",
    "    model_graph = {metric: metric_graph.dependency_graph[metric] for metric in metrics}\n",
    "    model_fns = {metric: calculated_model[metric] for metric in metrics}\n",
    "    return MetricGraph(model_graph, model_fns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spg_names = [f'{metric}_spg' for metric in pitching_metrics + hitting_metrics]\n",
    "\n",
    "fwar_model = {\n",
    "    'pitcher_raw_fWAR': (StatCalculator.reduce_sum, tuple(f'{metric}_spg' for metric in pitching_metrics)),\n",
    "    'pitcher_fWAR': (lambda fwar: fwar - largest(fwar, 160)[0], ('pitcher_raw_fWAR',)),\n",
    "    'pitcher_fWAR150': (lambda raw, ip: np.divide(raw * 150., ip, where=(ip!= 0.), out=np.zeros_like(ip, dtype=np.float32)), ('pitcher_raw_fWAR', 'IP')),\n",
    "    'hitter_raw_fWAR': (StatCalculator.reduce_sum, ('R_spg', 'RBI_spg', 'HR_spg', 'BA_spg', 'SB_spg')),\n",
    "    'hitter_fWAR': (partial(position_adjusted, pos='C', pos_roster=16, roster_depth=176), ('hitter_raw_fWAR', 'Eligible')),\n",
    "    'hitter_fWAR600':(lambda raw, pa: np.divide(raw * 600, pa, where=(pa!= 0.), out=np.zeros_like(pa, dtype=np.float32)), ('hitter_raw_fWAR', 'PA')),\n",
    "}\n",
    "\n",
    "fantasy_stat_model = StatCalculator.from_model(\n",
    "    extract_model(standings.assign(AB=lambda _: 5600, IP=lambda _: 1200), spg_model, spg_names).model()\n",
    "    | spg_model.model(['xER', 'xH', 'xWHIP'])\n",
    "    | fwar_model\n",
    ")\n",
    "median_stats = spg_model.calculate_metrics(standings, ['median_BA', 'median_ERA', 'median_WHIP'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbs_hitter_proj = fantasy_stat_model.add_metrics(cbs_hitter_proj.assign(median_BA=lambda _: median_stats['median_BA']), metrics=['HR_spg', 'BA_spg', 'hitter_fWAR', 'hitter_fWAR600'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbs_hitter_proj.sort_values('hitter_fWAR').tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbs_pitchers_proj = fantasy_stat_model.add_metrics(\n",
    "    cbs_pitchers_proj.assign(\n",
    "        median_ERA=lambda _: median_stats['median_ERA'],\n",
    "        median_WHIP=lambda _: median_stats['median_WHIP']\n",
    "    ),\n",
    "    metrics=['pitcher_fWAR', 'pitcher_fWAR150']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fantasy_stat_model.get_metric_dependencies(['hitter_fWAR']))\n",
    "#fantasy_stat_model.add_metrics(cbs_hitter_proj, ['hitter_fWAR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def position_value(hitter_fwar: ArrayLike, eligible: ArrayLike, pos: str, n_rostered: int) -> float:\n",
    "    eligible_idx = np.nonzero(matches_eligible(eligible, pos))\n",
    "    best = np.argpartition(hitter_fwar[eligible_idx], kth=n_rostered)[-n_rostered:]\n",
    "    return np.min(best)\n",
    "\n",
    "def matches_eligbible(eligbible: ArrayLike, pos: str) -> np.ndarray:\n",
    "    elig_series = pd.Series(eligbible)\n",
    "    pattern = f'^{pos}$|,{pos},|^{pos},|,{pos}$'\n",
    "    clean_elig = elig_series.str.replace(r'\\s', '', regex=True)\n",
    "    return clean_elig.str.contains(pattern, regex=True)\n",
    "\n",
    "def test_matches_eligible():\n",
    "    \"\"\"Test of matches_eligible.\"\"\"\n",
    "    eligible = ['C', 'C, CF', '1B, C', '1B, C, SS', 'CF', 'SS']\n",
    "    result = matches_eligbible(eligible, 'C')\n",
    "    assert np.array_equal(result, [True, True, True, True, False, False])\n",
    "\n",
    "test_matches_eligible()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Closers\n",
    "\n",
    "Which live in their own separate world."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix Edwin Diaz\n",
    "cbs_pitchers_proj.loc[15, 'playerid'] = 14710"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbs_pitchers_proj.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_ids(original_id) -> int:\n",
    "    \"\"\"Fix the datatype where some there are alpha numeric and integers in the id column.\"\"\"\n",
    "    if type(original_id) == str:\n",
    "        if re.search(r'[A-Za-z].', original_id):\n",
    "            id = int(re.sub(r'[A-Za-z].', '', original_id)) * 1\n",
    "            return id\n",
    "    elif pd.isna(original_id):\n",
    "        return -1\n",
    "    return int(original_id)\n",
    "\n",
    "\n",
    "def cummulative_rank(df):\n",
    "    # TODO: less lazy version where we at least pass the rank column names.\n",
    "    stacked = np.stack([df[col] for col in ('Eno', 'Greg', 'lenhart_rank')], axis=1)\n",
    "    stacked = np.where(stacked <= 40, 40 - stacked, 0.)\n",
    "    stacked = np.sqrt(np.sum(stacked ** 2, axis=1))\n",
    "    return rankdata(-1*stacked)\n",
    "\n",
    "closer_data = (\n",
    "    (athlethic_projections := projections.load_csv('the_athletic_closers.csv'))\n",
    "\n",
    "    .merge(\n",
    "        projections.load_csv('fg_depth_pitchers.csv'),\n",
    "        how='left',\n",
    "        on='Name',\n",
    "    )\n",
    "    .assign(playerid=lambda df: df.playerid.apply(lambda s: fix_ids(s)))#.astype({'int)\n",
    "    .filter(regex=r'Name|Eno|Greg|ADP|playerid')\n",
    "    .astype({'playerid': int})\n",
    "    .merge(cbs_pitchers_proj.filter(pitching_metrics + ['pitcher_fWAR', 'playerid']), on='playerid', how='left')\n",
    "    .dropna()\n",
    "    .assign(\n",
    "        lenhart_rank=lambda df: rankdata(df.pitcher_fWAR.values * -1),\n",
    "        closer_rank=cummulative_rank\n",
    "    )\n",
    "    .filter(['playerid', 'closer_rank', 'Eno', 'Greg'])\n",
    "    # .sort_values('closer_rank')\n",
    ")\n",
    "\n",
    "cbs_pitchers_proj.merge(closer_data, on='playerid', how='left').fillna(-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hitter_essentials = ['Avail', 'Player', 'Age', 'Eligible', 'PA', 'R', 'HR', 'RBI', 'K', 'SB', 'BA', 'hitter_fWAR', 'hitter_fWAR600']\n",
    "pitcher_essentials = ['Avail', 'Player', 'IP', 'GS', 'QS', 'W', 'S', 'K', 'ERA', 'WHIP', 'Rank', 'playerid', 'ER', 'pitcher_fWAR']\n",
    "\n",
    "hitter_df = cbs_hitter_proj[hitter_essentials]\n",
    "pitcher_df = cbs_pitchers_proj[pitcher_essentials]\n",
    "\n",
    "def filter_avail(df, regex):\n",
    "    return df[df.Avail.str.contains(regex)]\n",
    "\n",
    "def filter_elig(df, position):\n",
    "    return df[matches_eligible(df.Eligible, position)]\n",
    "\n",
    "all_players = (\n",
    "    pd.concat([pitcher_df, hitter_df])\n",
    "    .assign(fWAR=lambda df: np.where(df.hitter_fWAR.isna(), df.pitcher_fWAR.values, df.hitter_fWAR.values))\n",
    "    .query('fWAR > -1.')\n",
    "    .sort_values('fWAR', ascending=False)\n",
    "    .fillna(0)\n",
    ")\n",
    "hitter_df.query('hitter_fWAR > -.1 and hitter_fWAR < .1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbs_hitter_proj[hitter_essentials].pipe(filter_avail, '^Omak')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 0\n",
    "end = start + 20\n",
    "pitcher_df[['Avail', 'Player', 'pitcher_fWAR', 'S', 'ERA', 'W']].sort_values('pitcher_fWAR', ascending=False)[start:end].pipe(filter_avail, 'W\\W')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cbs_pitchers_proj.columns\n",
    "cbs_hitter_proj[hitter_essentials].pipe(filter_avail, '^W[^a-z]')\n",
    "cbs_hitter_proj[hitter_essentials].pipe(filter_elig, 'CI').pipe(filter_avail, '^W[^a-z]').head(25)[['Avail', 'Player', 'Eligible', 'hitter_fWAR', 'Rank']].sort_values('hitter_fWAR', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positions = ['C', '1B', '2B', '3B', 'SS', 'CF', 'LF', 'RF', 'U']\n",
    "for position in positions:\n",
    "    position_df = (\n",
    "        cbs_hitter_proj\n",
    "        .pipe(filter_elig, position)\n",
    "        .sort_values('hitter_fWAR', ascending=False)\n",
    "        .filter(hitter_essentials)\n",
    "        .pipe(filter_avail, '^W[^a-z]')\n",
    "        .query('hitter_fWAR > .1 or (hitter_fWAR600 > 15 and PA > 400)')\n",
    "        #.drop(columns='Avail')\n",
    "        .round(3)\n",
    "    )\n",
    "position_df#.to_csv(os.path.join(data_path, 'draft_2024/hitters_cs.csv'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_name(df, regex):\n",
    "    return df[df.Player.str.contains(regex, re.IGNORECASE)]\n",
    "\n",
    "filter_name(cbs_hitter_proj, 'Carter')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matching Unfound Players\n",
    "\n",
    "Everything above is solid. WIP stuff to find more player ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fg_depth = projections.load_csv('fg_depth_pitchers.csv')\n",
    "name = ''\n",
    "fg_depth.query('Name.str.contains(\"Edw\")')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbs_hitter_proj.query('playerid == -1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbs_pitchers_proj.query(\"Player.str.contains('Edw')\")\n",
    "cbs_pitchers_proj.query('playerid == -1 and IP > 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "france = unfound_hitters.loc[93, 'Player'] # in hitter_ids\n",
    "cbs_player_col_to_df(unfound_hitters.Player)#.merge()\n",
    "cbseam = cbs_player_col_to_df(cbs22.Player).Team.fillna('---').unique().tolist()\n",
    "teams = list(zip(\n",
    "    sorted(cbseam),\n",
    "    sorted(fg22.Team.unique().tolist())\n",
    "))\n",
    "{b: a for a, b in teams if a != b}"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "02ee4401c5460a4ee3bc94108cf95546c8e39c298a8555b54bdd3e60e7d4869e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
